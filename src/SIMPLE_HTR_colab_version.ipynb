{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SIMPLE/HTR <- colab version"
      ],
      "metadata": {
        "id": "BFuRfy0kx9Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import datasets\n",
        "\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "from io import BytesIO\n",
        "\n",
        "\"\"\"\n",
        "IAM Dataset download\n",
        "\"\"\"\n",
        "\n",
        "if not os.path.exists('/content/data/') :\n",
        "    os.mkdir(\"/content/data\")\n",
        "\n",
        "splits = {'train': 'data/train.parquet', 'validation': 'data/validation.parquet', 'test': 'data/test.parquet'}\n",
        "\n",
        "\n",
        "max_len = -1\n",
        "str_len = -1\n",
        "for iter in ['test', 'validation', 'train']:\n",
        "\n",
        "    if not os.path.exists(f'/content/data/{iter}') :\n",
        "        os.mkdir(f\"/content/data/{iter}\")\n",
        "    df = pd.read_parquet(\"hf://datasets/Teklia/IAM-line/\" + splits[iter])\n",
        "    df_csv = df['text']\n",
        "\n",
        "    if iter == 'train':\n",
        "        char_set = set()\n",
        "        for label in df_csv:\n",
        "            char_set.update(label)\n",
        "        char_set = list(char_set)\n",
        "        char_set.sort()\n",
        "\n",
        "        cToi = {}\n",
        "        iToc = {}\n",
        "        for index_, char in enumerate(char_set):\n",
        "            cToi.update({char : index_ +1})\n",
        "            iToc.update({index_ +1 : char})\n",
        "\n",
        "        # save data\n",
        "        np.save(\"char_set.npy\",char_set)\n",
        "        np.save('cToi.npy', cToi)\n",
        "        np.save('iToc.npy', iToc)\n",
        "\n",
        "\n",
        "\n",
        "    with open(os.path.join(\"/content/data\",f\"{iter}.csv\"), 'w', newline=\"\") as csvfile:\n",
        "        spanwriter = csv.writer(csvfile, delimiter = '\\t', quotechar=\"|\")\n",
        "        spanwriter.writerow([\"_path\",\"text\"])\n",
        "        for id, txt in enumerate(df_csv):\n",
        "            spanwriter.writerow([id,txt])\n",
        "            if len(txt) > str_len:\n",
        "                str_len =len(txt)\n",
        "\n",
        "    #df_csv.to_csv(path_or_buf=os.path.join('/content/data', f'{iter}.csv'), sep='\\t')\n",
        "\n",
        "    df_imag = df['image']\n",
        "    for idx, img in enumerate(df_imag):\n",
        "        image = Image.open( BytesIO(img['bytes']))\n",
        "\n",
        "        image.save(os.path.join(f\"./data/{iter}\",f\"{idx}.jpg\" ))\n",
        "\n",
        "        width_size , height_size = image.size\n",
        "        if(width_size > max_len):\n",
        "            max_len =width_size\n",
        "\n",
        "        if(height_size != 128):\n",
        "            print(height_size)\n",
        "\n",
        "print(max_len)\n",
        "print(str_len)"
      ],
      "metadata": {
        "id": "iKuaE89cyDiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#pre-processing\n",
        "retain aspect ratio of images and use batches of padded images in order to effectively use mini-batch Stochastic Gradient Descent(SGD)\n"
      ],
      "metadata": {
        "id": "J0YcuXaFyXi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All images are resized to a resulution of 128x1024 pixels for line images or 64x256 pixels.\n",
        "# Initial images are padded in order to attain the aformentioned fixed size.\n",
        "\n",
        "# IAM dataset image have a size width x 128(height), the max width size = 5027\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# target_size : 1024 x 128\n",
        "def preprocess(image) :\n",
        "    max_height = 128\n",
        "    max_width = 1024\n",
        "    #image = image.convert(\"L\")\n",
        "\n",
        "    width_size, height_size = image.size\n",
        "    if (height_size > 128):\n",
        "        print(width_size, height_size)\n",
        "        ratio = 128/height_size\n",
        "        #image = image.resize((math.ceil(width_size*ratio), 128))\n",
        "        image = F.resize(image , ( 128,math.ceil(width_size*ratio)))\n",
        "        width_size, height_size = image.size\n",
        "\n",
        "    if (width_size > 1024):\n",
        "        ratio = 1024/width_size\n",
        "        #image = image.resize( (1024, math.ceil(height_size *ratio) ) )\n",
        "        image =F.resize(image , (math.ceil(height_size *ratio) ,1024))\n",
        "    image = np.array(image)\n",
        "    image = torch.tensor(image)\n",
        "    image = padd_img(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "    # padd it 1024x128\n",
        "def padd_img(img):\n",
        "    height_tar, width_tar = 128,1024\n",
        "    height_size, width_size = img.shape\n",
        "\n",
        "    ## 128 - 51 = 77\n",
        "    ## 77/2 = > 38  76 + 51 =127\n",
        "\n",
        "    left = math.floor((width_tar - width_size)/2)\n",
        "    right =math.floor((width_tar -width_size)/2)\n",
        "    if(width_size %2 == 1):\n",
        "        right = math.floor((width_tar - width_size)/2) +1\n",
        "    top = math.floor((height_tar - height_size)/2)\n",
        "    bottom = math.floor((height_tar - height_size)/2)\n",
        "    if (height_size %2 ==1):\n",
        "        bottom = math.floor((height_tar - height_size)/2) + 1\n",
        "\n",
        "    img = nn.functional.pad(img,(left,right,top,bottom),mode=\"constant\", value=255)\n",
        "    return img\n",
        "\n",
        "\n",
        "#During traing, image augmentation is performed.\n",
        "#considering only rotation and skew of small magnitude in order to generate valid images\n",
        "#Additionaly, gaussian nosie is added to the images\n",
        "from torchvision.transforms import v2\n",
        "\"\"\"\n",
        "transforms = v2.RandomApply(\n",
        "    torch.nn.ModuleList([\n",
        "        v2.RandomAffine(degrees=(-10,10),translate=(0.01,0.05), scale = (0.8,1.2),shear = (-10,10))\n",
        "        ]) , p=0.5 )\n",
        "\"\"\"\n",
        "transforms = v2.RandomApply(\n",
        "    torch.nn.ModuleList([\n",
        "        v2.RandomAffine(degrees = (0,0),translate=(0.01,0.05), scale = (0.8,1),shear = (-10,10),fill=255)\n",
        "        ]) , p=0.5 )\n",
        "\n",
        "# rotation image ... are cutted so did;nt show the character in image ...\n",
        "# even i didn't see the chacracter it is... not good\n",
        "# rotation, translation, scaling and shearing  and gray-scale erosion and dilation\n",
        "# the paper named \"Are Multidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\" said it toolkit defualt value but didn't found it\n",
        "\n",
        "\"\"\"\n",
        "We perform adequate random distortions on the input images,\n",
        "in order to artificially augment the training samples and reduce overfitting.\n",
        "\n",
        "These distortions include: rotation,translation, scaling and shearing\n",
        "(all performed as a single affine transform) and gray-scale erosion and dilation.\n",
        "Each of these operations is applied dynamically and independently on each image of the training batch (each with 0.5 probability).\n",
        "Thus, the exact same image is virtually never observed twice\n",
        "during training.\n",
        "The parameters controlling each distortion (e.g. rotation angle, scaling factor, erosion kernel, etc.) are sampled from a fixed distribution.\n",
        "    -- Are Multidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\n",
        "\"\"\"\n",
        "# transforms  => preprocessing\n",
        "\n",
        "\n",
        "#Each word/line transcription has spaces added before and after, \"He rose from\" => \" He rose from \"\n",
        "\n",
        "def pre_processing_target(label):\n",
        "    return \" \" + label + \" \"\n",
        "target_transforms = pre_processing_target\n",
        "\n",
        "#This operation aims to assist the system during the training phase, For the testing phase, these additional spaces are discarded\n"
      ],
      "metadata": {
        "id": "C9BQXO6-ztlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "class IAMData():\n",
        "    def __init__(self,split ,root_path = '/content/data' , transform =None,target_transform = None ):\n",
        "\n",
        "        assert os.path.exists(root_path), \"valueError: IAMData 'wrong root_path' \"\n",
        "\n",
        "        assert  split in ['train', 'test', 'validation'] , \"valueError: IAMData 'split must be one of ['train', 'test', 'validation]'\"\n",
        "\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "\n",
        "        self.annotations_file = os.path.join(root_path, f\"{split}.csv\")\n",
        "        self.img_dir = os.path.join(root_path, split)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        self.img_labels = pd.read_csv(self.annotations_file, delimiter = '\\t', quotechar='|')\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        img_path = os.path.join(self.img_dir, f\"{self.img_labels.iloc[idx,0]}.jpg\")\n",
        "\n",
        "        image = Image.open(img_path)\n",
        "        label =self.img_labels.iloc[idx,1]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        image = self.preprocess(image)\n",
        "        image = torch.Tensor(image).float().unsqueeze(0).reshape(1,128,1024)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "UhRrwJ_g6DaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#architectural\n",
        "replace the column-wise concatenation step between the CNN backbone and the recurrent head with a max-pooling step. such a choice not only redueces the required parameters but has an intuitive motivation: we care only about the existence of a character and not its vertical poision"
      ],
      "metadata": {
        "id": "osjkL5RhymeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolutional Backbone"
      ],
      "metadata": {
        "id": "vCetWnOzPvwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In our model, the convolutional backbone is made\n",
        "up of standard convolutional layers and ResNet blocks [12], interspersed with\n",
        "max-pooling and dropout layers.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.nn import functional\n",
        "\n",
        "class Residual_Block(nn.Module):\n",
        "    def __init__(self,in_channels, out_channels,stride =1):\n",
        "        super(Residual_Block,self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = functional.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = functional.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ConvolutionalBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvolutionalBackbone, self).__init__()\n",
        "\n",
        "        self.CNNLayer = nn.Conv2d(1,32,(7,7), stride=1 , padding=3)\n",
        "\n",
        "        self.MaxPoolFirst = nn.MaxPool2d((2,2))\n",
        "        self.ResBlockFirst = nn.Sequential(\n",
        "              Residual_Block(32,64),\n",
        "              Residual_Block(64,64)\n",
        "        )\n",
        "\n",
        "        self.MaxPoolSecond = nn.MaxPool2d((2,2))\n",
        "        self.ResBlockSecond = nn.Sequential(\n",
        "              Residual_Block(64,128),\n",
        "              Residual_Block(128,128),\n",
        "              Residual_Block(128,128),\n",
        "              Residual_Block(128,128)\n",
        "        )\n",
        "\n",
        "        self.MaxPoolThird = nn.MaxPool2d((2,2))\n",
        "        self.ResBlockThird = nn.Sequential(\n",
        "              Residual_Block(128,256),\n",
        "              Residual_Block(256,256),\n",
        "              Residual_Block(256,256),\n",
        "              Residual_Block(256,256)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.reshape([-1,1,128,1024])\n",
        "        #[128,1024]\n",
        "        logits = self.CNNLayer(x)\n",
        "        logits = self.MaxPoolFirst(logits)\n",
        "        logits = self.ResBlockFirst(logits)\n",
        "\n",
        "        logits = self.MaxPoolSecond(logits)\n",
        "        logits = self.ResBlockSecond(logits)\n",
        "\n",
        "        logits = self.MaxPoolThird(logits)\n",
        "        logits = self.ResBlockThird(logits)\n",
        "\n",
        "        #[batch, channel, height, width ] =[batch, 256,16,128]\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# the first layer is a 7x7 convolution with 32 output channels\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "l-hl2HulO5qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvolutionalBackbone()\n",
        "\n",
        "\n",
        "a = torch.rand(2,1,128,1024)\n",
        "a = model(a)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "id": "Cdz6ZSzecNmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Flattening Operation => ColumneWiseMaxPooling"
      ],
      "metadata": {
        "id": "Rl1GM3mUPzb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    The convolutional backbone output should be transformed into a sequence of features in order to processed by recurrent networks.\n",
        "    Typical HTR approaches, assume a column-wise approach (towards the writing direction) to ideally simulate a character by character processing.\n",
        "    the CNN output is flattened by a max-pooling operation in a column-wise manner\n",
        "\n",
        "    ... Apart from the apparent computational advantage,\n",
        "    column-wise max-pooling achieves model translation invariance in the vertical direction\n",
        "\"\"\"\n",
        "\n",
        "#.. it is too small, it is better to apply it just function ... in whole class ...\n",
        "class ColumnwiseMaxPool(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColumnwiseMaxPool,self).__init__()\n",
        "\n",
        "    def forward(self,CNNoutput):\n",
        "        MaxPoolData  =torch.max(CNNoutput, dim = 2)\n",
        "        #[batch,channel,width]\n",
        "        return MaxPoolData"
      ],
      "metadata": {
        "id": "opKVmzfqP2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recurrent Head\n"
      ],
      "metadata": {
        "id": "YOQMqwPyP2r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    The recurrent component consists of 3 stacked Bidirectional Long Short-Term Memory (BiLSTM) units of hidden size 256. These are followed\n",
        "by a linear projection layer, which converts the sequence to a size equal to the number of possible character tokens, nclasses (including the blank character, required by CTC).\n",
        "\n",
        "    The final output of the recurrent part can be translated into a sequence of probability distributions by applying a softmax operation.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        " During evaluation, the aforementioned greedy decoding is performed by selecting the character\n",
        "  with the highest probability at each step and then removing the blank characters from the resulting sequence [8].\n",
        "\"\"\"\n",
        "\n",
        "class RecurrentHead(nn.Module):\n",
        "    def __init__(self, nclasses):\n",
        "        super(RecurrentHead,self).__init__()\n",
        "        self.BiLSTM = nn.LSTM(256,256,num_layers=3,bidirectional=True)\n",
        "        self.Projection = nn.Linear(512,nclasses)\n",
        "        #nclasses include the blank character\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.permute(2,0,1)\n",
        "        #[batch,channel, width] = > [width,batch,channel]\n",
        "        outputs, (hidden , cell) = self.BiLSTM(x)\n",
        "        logits = self.Projection(outputs)\n",
        "        #[width,batch,2 * channel] => [width,batch, nclasses ]\n",
        "        logits = logits.transpose(0,1)\n",
        "        #[batch,width,nclasses]\n",
        "\n",
        "        return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "H7yqXOaIP4WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CTC shortcut\n"
      ],
      "metadata": {
        "id": "4dzfCOmZoHjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Architecture-wise, the CTC shortcut module consists only of\n",
        "a single 1D convolutional layer, with kernel size 3. Its output channels equal\n",
        "to the number of the possible character tokens (nclasses). Therefore, the 1D\n",
        "convolutional layer is responsible for straightforwardly encoding context-wise\n",
        "information and providing an alternative decoding path.\n",
        "\"\"\"\n",
        "\n",
        "class CTCshortcut(nn.Module):\n",
        "    def __init__(self, nclasses):\n",
        "        # nclasses include the black_id\n",
        "        super(CTCshortcut,self).__init__()\n",
        "\n",
        "        self.ConLayer = nn.Conv1d(256,nclasses,3,padding = 1)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        logits = self.ConLayer(x)\n",
        "        #[batch,channel,width]=>[batch,nclasses,width]\n",
        "        logits = logits.transpose(1,2)\n",
        "        #[batch,nclasses,width] => [batch,width,nclasses]\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "VPkqAdFWoJn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CTC Loss\n",
        "\n",
        "the multi-task loss is written as\n",
        "\n",
        "$L_{CTC}(f_{rec}(f_{cnn}(I)) ;s ) +0.1 L_{CTC}((f_{shortcut}(f_{cnn}(I)); s))$\n",
        "\n",
        "\n",
        "\n",
        "    The CTC shortcut is trained along with the main architecture using\n",
        "    a multitask loss by adding the corresponding CTC losses of the two branches with the appropriate weights\n",
        "\n",
        "    Since CTC shortcut acts only as an auxiliary training path, it is weighted by 0.1\n",
        "    to reduce its relative contribution to the overall loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "CCQqKWCDs96E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#total HTRnet\n",
        "\n",
        "class HTRnet(nn.Module):\n",
        "    def __init__(self, nclasses):\n",
        "        #nclasses include the blank_id\n",
        "        super(HTRnet,self).__init__()\n",
        "\n",
        "        self.CNNBackbone = ConvolutionalBackbone()\n",
        "        # columnewise Maxpooling\n",
        "\n",
        "        self.RecHead = RecurrentHead(nclasses)\n",
        "        self.CTCshort = CTCshortcut(nclasses)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.CNNBackbone(x)\n",
        "        #[batch,channel,height, width]\n",
        "\n",
        "        # columnewise Maxpooling\n",
        "        colMaxPool = torch.max(features,dim =2).values\n",
        "        #[batch,channel, width]\n",
        "\n",
        "        RecOutput =  self.RecHead(colMaxPool)\n",
        "        CTCOutput = self.CTCshort(colMaxPool)\n",
        "        #[batch,width, nclasses]\n",
        "\n",
        "        return RecOutput, CTCOutput\n"
      ],
      "metadata": {
        "id": "oOz18xAYxJWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HTRnet(93)\n",
        "a = torch.rand(2,1,128,1024)\n",
        "Rec, CTC = model(a)\n",
        "print(Rec.shape)\n",
        "print(CTC.shape)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tAApuuwIz23D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix\n",
        "\n",
        "it is copy of HTR-best-practices\n",
        "\n",
        "https://github.com/georgeretsi/HTR-best-practices/blob/main/utils/metrics.py"
      ],
      "metadata": {
        "id": "ijrAs2dEbF0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import editdistance\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# character error rate\n",
        "class CER:\n",
        "    def __init__(self):\n",
        "        self.total_dist = 0\n",
        "        self.total_len = 0\n",
        "\n",
        "    def update(self, prediction, target):\n",
        "        dist = float(editdistance.eval(prediction, target))\n",
        "        self.total_dist += dist\n",
        "        self.total_len += len(target)\n",
        "\n",
        "    def score(self):\n",
        "        return self.total_dist / self.total_len\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_dist = 0\n",
        "        self.total_len = 0\n",
        "\n",
        "# word error rate\n",
        "# two supported modes: tokenizer & space\n",
        "class WER:\n",
        "    def __init__(self, mode='tokenizer'):\n",
        "        self.total_dist = 0\n",
        "        self.total_len = 0\n",
        "\n",
        "        if mode not in ['tokenizer', 'space']:\n",
        "            raise ValueError('mode must be either \"tokenizer\" or \"space\"')\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "    def update(self, prediction, target):\n",
        "        if self.mode == 'tokenizer':\n",
        "            target = word_tokenize(target)\n",
        "            prediction = word_tokenize(prediction)\n",
        "        elif self.mode == 'space':\n",
        "            target = target.split(' ')\n",
        "            prediction = prediction.split(' ')\n",
        "\n",
        "        dist = float(editdistance.eval(prediction, target))\n",
        "        self.total_dist += dist\n",
        "        self.total_len += len(target)\n",
        "\n",
        "    def score(self):\n",
        "        return self.total_dist / self.total_len\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_dist = 0\n",
        "        self.total_len = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "6V_oiZCxbJiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(tdec, tdict, blank_id=0):\n",
        "\n",
        "    tt = [v for j,v in enumerate(tdec) if j==0 or v !=tdec[j-1]]\n",
        "    dec_transcr = ''.join([tdict[t] for t in tt if t != blank_id])\n",
        "\n",
        "    return dec_transcr\n",
        "\n",
        "a = [[1,2,3,5,7,5,8,9,2], [2,7,6,4,7,43,5,6,5]]\n",
        "iTOc = np.load('iToc.npy',allow_pickle=True).tolist()\n",
        "print(iTOc)\n",
        "dec_transcr = decoder(a,iTOc)\n",
        "print(dec_transcr)"
      ],
      "metadata": {
        "id": "CsUhhkENh1fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iTOc = np.load('iToc.npy', allow_pickle=True)\n",
        "print(iTOc)\n",
        "iTOc = iTOc.tolist()\n",
        "print(iTOc)"
      ],
      "metadata": {
        "id": "yO-LR4jpkQi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training\n",
        "add and extra shortcut branch, consisting of a single 1D convoluion layer, at the ouptput of the CNN backbone. this branch results to a an extra character sequnce estimation, trained in parallel to the recurrent branch. both branches use a CTC loss. the motivation behind such a choice comes from the increased difficulty of training recurrent layers. However, if such a strainghtforward shortcut exists, the output of the CNN backbone should coverge to more discriminative features, ideal for fully harnessing the power of recurrent layers compared to an end-to-end training scheme"
      ],
      "metadata": {
        "id": "BeotOKjUy6rV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  the training of the HTR system is performed via an Adam optimizer\n",
        "  using an initial learing rate of  0.001 which gradually decreases using a multistep schedular.\n",
        "  the overall training epochs are 240 and the scheduler decreses the learning rate by a factor of 0.1 at 120 and 180 epoches\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "  the optimizering sheme, with minor modifications, is commonly used for HTR systems.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import tqdm\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "class HTRTrainer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HTRTrainer,self).__init__()\n",
        "\n",
        "        self.prepare_dataloaders()\n",
        "        self.prepare_net()\n",
        "        self.prepare_losses()\n",
        "        self.prepare_optimizers()\n",
        "\n",
        "    def prepare_dataloaders(self):\n",
        "        train_ds = IAMData('train',transform = transforms,target_transform= target_transforms)\n",
        "        test_ds = IAMData('test')\n",
        "        validation_ds = IAMData('validation')\n",
        "\n",
        "        self.char_set = np.load('char_set.npy')\n",
        "        self.cToi = np.load('cToi.npy', allow_pickle=True).tolist()\n",
        "        self.iToc = np.load('iToc.npy', allow_pickle=True).tolist()\n",
        "\n",
        "        train_loader = DataLoader(train_ds,batch_size = 16, shuffle=True)\n",
        "        validation_loader = DataLoader(validation_ds,batch_size = 16, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds,batch_size = 16, shuffle=True)\n",
        "\n",
        "        self.loaders = {'train':train_loader ,'test': test_loader,'validation': validation_loader}\n",
        "\n",
        "    def prepare_net(self):\n",
        "        nclasses = len(self.char_set)+1\n",
        "\n",
        "        self.net = HTRnet(nclasses)\n",
        "\n",
        "    def prepare_losses(self):\n",
        "        self.CTCLoss = nn.CTCLoss(reduction='sum')\n",
        "        # the Log_probs of input nn.functional.log_softmax()\n",
        "\n",
        "    def prepare_optimizers(self):\n",
        "        \"\"\"\n",
        "          This optimizing scheme, with minor modifications, is commonly used for HTR systems.\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "          The overall training epochs are 240 and\n",
        "          the scheduler decreases the learning rate by a factor of 0.1 at 120 and 180 epochs\n",
        "        \"\"\"\n",
        "        self.optimizer = torch.optim.AdamW(self.net.parameters(), 0.001,weight_decay=0.00005)\n",
        "        self.schedular = torch.optim.lr_scheduler.MultiStepLR(self.optimizer,[120,180], gamma=0.1)\n",
        "\n",
        "    def decoder(self,tdecs, tdict, blank_id=0):\n",
        "\n",
        "        tt= [t for j, t in enumerate(tdecs) if j == 0 or t!=tdecs[j-1] ]\n",
        "        dec_transcr = \"\".join([tdict[t] for t in tt if t != blank_id])\n",
        "\n",
        "        return dec_transcr\n",
        "\n",
        "    def encoder(self, labels, tdict, blank_id = 0):\n",
        "\n",
        "        enc_transcrs = []\n",
        "        for label in labels:\n",
        "            enc_transcr = []\n",
        "            for c in label :\n",
        "                enc_transcr.append(tdict[c])\n",
        "            enc_transcrs.append(enc_transcr)\n",
        "\n",
        "        return enc_transcrs\n",
        "\n",
        "    def padd_transcr(self, labels, max_length, blank_id =0):\n",
        "\n",
        "        result=[]\n",
        "        for label in labels:\n",
        "            label = torch.tensor(label)\n",
        "            out = torch.nn.functional.pad(label, (0,max_length -len(label)), mode = 'constant',value=blank_id )\n",
        "            result.append(out)\n",
        "\n",
        "        result = torch.stack(result,dim=0)\n",
        "        return result\n",
        "\n",
        "    def max_length(self, labels):\n",
        "        max_length_ =-1\n",
        "        for label in labels:\n",
        "            if len(label)> max_length_:\n",
        "                max_length_ = len(label)\n",
        "\n",
        "        return max_length_\n",
        "\n",
        "    def target_lengths(self, labels):\n",
        "        target_lengths_ =[]\n",
        "        for label in labels:\n",
        "            target_lengths_.append(len(label))\n",
        "\n",
        "        target_lengths_ = torch.tensor(target_lengths_)\n",
        "        return target_lengths_\n",
        "\n",
        "    def sample_decoding(self):\n",
        "        # get a random image from the test set\n",
        "\n",
        "        img, transcr = self.loaders['validation'].dataset[np.random.randint(0,len(self.loaders['validation'].dataset))]\n",
        "        self.net.eval()\n",
        "        self.net.cpu()\n",
        "\n",
        "        #plt.figure()\n",
        "        #plt.imshow(to_pil_image(img.numpy()), cmap = 'gray')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            tst_o, __ = self.net(img)\n",
        "\n",
        "        tdec = tst_o.argmax(2).cpu().numpy()\n",
        "        tdec = tdec[0]\n",
        "\n",
        "        dec_transcr = self.decoder(tdec, self.iToc)\n",
        "        target = transcr.strip()\n",
        "        prediction = dec_transcr.strip()\n",
        "\n",
        "        print('orig:: ' + target)\n",
        "        print('pred:: ' + prediction)\n",
        "\n",
        "        dist = float(editdistance.eval(prediction, target))\n",
        "        length = len(target)\n",
        "        print('dist:: {}'.format(dist/length))\n",
        "\n",
        "    def load_model(self, _path):\n",
        "\n",
        "        #self.net.load_state_dict(torch.load(_path))\n",
        "        self.net.load_state_dict(torch.load(_path, map_location=torch.device('cpu') ) )\n",
        "\n",
        "    def test(self, epoch, tset = 'test'):\n",
        "        self.net.eval()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        if tset == 'test':\n",
        "            loader = self.loaders['test']\n",
        "        elif tset == 'validation':\n",
        "            loader = self.loaders['validation']\n",
        "\n",
        "        else:\n",
        "            print('not recognized set in test function')\n",
        "\n",
        "        print('####################### Evaluating {} set at epoch {} #######################'.format(tset, epoch))\n",
        "\n",
        "        cer, wer = CER(), WER(mode='tokenizer')\n",
        "        self.net.to(device)\n",
        "        for (imgs, transcrs) in tqdm.tqdm(loader):\n",
        "\n",
        "            imgs =imgs.to(device)\n",
        "            with torch.no_grad():\n",
        "                o, __  = self.net(imgs)\n",
        "\n",
        "            tdecs = o.argmax(2).cpu().numpy()\n",
        "\n",
        "            for tdec, transcr in zip(tdecs, transcrs):\n",
        "\n",
        "                transcr = transcr.strip()\n",
        "                dec_transcr = self.decoder(tdec, self.iToc).strip()\n",
        "\n",
        "                cer.update(dec_transcr, transcr)\n",
        "                wer.update(dec_transcr, transcr)\n",
        "\n",
        "        cer_score = cer.score()\n",
        "        wer_score = wer.score()\n",
        "\n",
        "        print('CER at epoch {}: {:.3f}'.format(epoch, cer_score))\n",
        "        print('WER at epoch {}: {:.3f}'.format(epoch, wer_score))\n",
        "\n",
        "\n",
        "    def train_one_epoch(self,epoch_index, device):\n",
        "        running_loss =0.\n",
        "        last_loss =0.\n",
        "\n",
        "        loader = self.loaders['train']\n",
        "        self.net.train()\n",
        "        iter=0\n",
        "        for (imgs, labels) in tqdm.tqdm(loader):\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            RecTrancr, CTCTranscr = self.net(imgs)\n",
        "            # we didn't do log_softmax yet\n",
        "            RecTdecs = torch.nn.functional.log_softmax(RecTrancr, dim = 2).transpose(0,1)\n",
        "\n",
        "            width_size, batch_size, nclasses = RecTdecs.shape\n",
        "            input_lengths = torch.full(size=(batch_size,), fill_value=width_size, dtype=torch.long)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "\n",
        "\n",
        "            labels = self.encoder(labels, self.cToi)\n",
        "            max_lengths = self.max_length(labels)\n",
        "            target_lengths = self.target_lengths(labels)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "\n",
        "            #padd labels to max_length\n",
        "            labels = self.padd_transcr(labels, max_length=max_lengths, blank_id = 0)\n",
        "            labels = labels.to(device)\n",
        "            loss = self.CTCLoss(RecTdecs, labels,input_lengths, target_lengths)\n",
        "            CTCTdecs = torch.nn.functional.log_softmax(CTCTranscr, dim = 2).transpose(0,1)\n",
        "            loss += self.CTCLoss(CTCTdecs,labels,input_lengths, target_lengths) * 0.1\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            self.schedular.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        last_loss = running_loss / 100 # loss per batch\n",
        "        running_loss = 0.\n",
        "\n",
        "        return last_loss\n",
        "\n",
        "    def train(self):\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        EPOCHS = 800\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.net.to(device)\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            print('EPOCH {}:'.format(epoch + 1))\n",
        "            self.net.train()\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            avg_loss = self.train_one_epoch(epoch, device)\n",
        "\n",
        "            self.net.eval()\n",
        "\n",
        "            print('LOSS train {}'.format(avg_loss))\n",
        "            self.test(epoch= epoch,tset='validation')\n",
        "\n",
        "            # Track best performance, and save the model's state\n",
        "            if epoch % 20 ==0 :\n",
        "                self.test(epoch = epoch, tset='test')\n",
        "                model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
        "                torch.save(self.net.state_dict(), model_path)\n",
        "\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
        "        torch.save(self.net.state_dict(), model_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ruAcm-qU1xUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer = HTRTrainer()\n",
        "Trainer.load_model(\"model_20250503_121842_140\")\n",
        "Trainer.sample_decoding()"
      ],
      "metadata": {
        "id": "4XQpUKc1JTuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    Trainer.sample_decoding()"
      ],
      "metadata": {
        "id": "_kpcyXGPLdQK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer.test(0)"
      ],
      "metadata": {
        "id": "ZZOp1QYHqK1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Trainer.train()"
      ],
      "metadata": {
        "id": "Ehc-3D5rEL4R",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "Y_168dVzH780"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HTREval(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HTREval,self).__init__()\n",
        "        self.prepare_dataloaders()\n",
        "        self.prepare_net()\n",
        "\n",
        "    def prepare_dataloaders(self):\n",
        "        train_ds = IAMData('train',transform = transforms,target_transform= target_transforms)\n",
        "        test_ds = IAMData('test')\n",
        "        validation_ds = IAMData('validation')\n",
        "\n",
        "        self.char_set = np.load('char_set.npy')\n",
        "        self.cToi = np.load('cToi.npy', allow_pickle=True).tolist()\n",
        "        self.iToc = np.load('iToc.npy', allow_pickle=True).tolist()\n",
        "\n",
        "        train_loader = DataLoader(train_ds,batch_size = 16, shuffle=True)\n",
        "        validation_loader = DataLoader(validation_ds,batch_size = 16, shuffle=True)\n",
        "        test_loader = DataLoader(test_ds,batch_size = 16, shuffle=True)\n",
        "\n",
        "        self.loaders = {'train':train_loader ,'test': test_loader,'validation': validation_loader}\n",
        "\n",
        "    def prepare_net(self):\n",
        "        nclasses = len(self.char_set)+1\n",
        "\n",
        "        self.net = HTRnet(nclasses)\n",
        "\n",
        "    def load_model(self, _path):\n",
        "        self.net.load_state_dict(torch.load(_path))\n",
        "\n",
        "    def decoder(self,tdec, tdict, blank_id=0):\n",
        "        tt = [v for j,v in enumerate(tdec) if j==0 or v !=tdec[j-1]]\n",
        "        dec_transcr = ''.join([tdict[t] for t in tt if t!=blank_id])\n",
        "\n",
        "        return dec_transcr\n",
        "\n",
        "\n",
        "    def test(self, epoch, tset = 'test'):\n",
        "        self.net.eval()\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        if tset == 'test':\n",
        "            loader = self.loaders['test']\n",
        "        elif tset == 'validation':\n",
        "            loader = self.loaders['validation']\n",
        "\n",
        "        else:\n",
        "            print('not recognized set in test function')\n",
        "\n",
        "        print('####################### Evaluating {} set at epoch {} #######################'.format(tset, epoch))\n",
        "\n",
        "        cer, wer = CER(), WER(mode='tokenizer')\n",
        "        for (imgs, transcrs) in tqdm.tqdm(loader):\n",
        "\n",
        "            imgs.to(device)\n",
        "            with torch.no_grad():\n",
        "                o, __ = self.net(imgs)\n",
        "\n",
        "            tdecs = o.argmax(2).permute(1, 0).cpu().numpy().squeeze()\n",
        "\n",
        "            for tdec, transcr in zip(tdecs, transcrs):\n",
        "                transcr = transcr.strip()\n",
        "                dec_transcr = self.decoder(tdec, self.iToc).strip()\n",
        "\n",
        "                cer.update(dec_transcr, transcr)\n",
        "                wer.update(dec_transcr, transcr)\n",
        "\n",
        "        cer_score = cer.score()\n",
        "        wer_score = wer.score()\n",
        "\n",
        "        print('CER at epoch {}: {:.3f}'.format(epoch, cer_score))\n",
        "        print('WER at epoch {}: {:.3f}'.format(epoch, wer_score))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8rE02wvyH7Q8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}