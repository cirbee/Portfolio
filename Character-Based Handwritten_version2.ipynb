{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOUB6gWDyyi9K8BEJggf17p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["##Character-Based Handwritten Text Transcription with\n","##Attention Networks version -2\n","\n","url:= https://arxiv.org/pdf/1712.04046v3"],"metadata":{"id":"dt4YE9wAkwbU"}},{"cell_type":"markdown","source":["# 1. Introduction\n","\n","an open problem : it is harder to segment and recognize individual charactors rather tahn words\n","\n"," Moreover, transcription models must solve the problem of finding and classifying characters at\n","each time-step without knowing the alignment between the input sequence of image pixels\n","and the target sequence of characters [2]."],"metadata":{"id":"tAo1SpuOlPNs"}},{"cell_type":"markdown","source":["Previous : RNN and CTC\n","\n","=> the encoder-decoder network proposed by Deng et al. which extends the encoder-decoder RNNs of Vinyals et al.\n","\n","\n","The encoder-decoder\n","model encodes a variable-length sequence of characters into a fixed-length vector and then\n","decodes the vector into a variable-length target label. Encoder-decoder RNNs are suitable for\n","handling long sequences of data and have become standard for neural machine translation,\n","speech recognition [12], and image captioning [13] tasks.\n"],"metadata":{"id":"StHbcU41k0Q7"}},{"cell_type":"markdown","source":["Encoder-decoder RNNs are suitable for\n","handling long sequences of data and have become standard for neural machine translation,\n","speech recognition [12], and image captioning [13] tasks.\n","\n","\n","CNN : extracts visual features from images,\n","\n","RNN Encoder :  re-encoders each row of the grid.\n","\n","RNN Decoder : output a character sequence one step at a time, using an attention mechanism to emphasize the most important columns of re-encoded features at each decoding step.\n","\n","\n","Attention-based networks are\n","capable of modeling the language structures within the output sequence, rather than simply\n","mapping the input to the correct output [15].\n","\n","\n","The main differentiator in our approach is that we employ a CNN to extract image features and a separate RNN encoder to re-encode the features so that the encoder can learn new\n","features such as text directionality. Another difference is that we use an unidirectional RNN\n","decoder to predict the sequence of characters. Gui et al. [29] train character-aware attention\n","networks, but the architecture differs in that they use an attention-based bidirectional RNN\n","decoder and CTC output layer to convert predictions made by the decoder into a character\n","sequence\n","\n"],"metadata":{"id":"_fkUUWlXp5uO"}},{"cell_type":"markdown","source":["Character-aware\n","models view the input and output text lines as a sequence of characters rather than words,\n","and each character prediction is explicitly conditioned on the previous character. These models are capable of making inferences about unseen source words and also generating unseen"],"metadata":{"id":"HoDeo6Q_tOJR"}},{"cell_type":"markdown","source":["CNN : inputs x and arrange the visual features on a grid, V with dimensions H x W x C  C: channels, H,W are reduced dimensions by max pooling operations\n","\n","Pool : Max pool\n","Bn : Batch normalization\n","\n","the RNN encoder slides across each row of V, and at each time-step t, recursively update a hidden state h_t, using v_t as input:\n","\n","  h_t = f(v_t, h_t-1; seata)\n","\n","  f : a non linear activation\n","  t : a each time-step\n","  h_t : a hidden state\n","\n","Encoding row-wise is useful for transcription tasks // the encoder can learn sequential order information // test directionally.\n","\n","the encoder outputs a re-encoded feature grid hat V_h,w\n","\n","Hat V := an inital hidden state, hat V_h,0\n","\n","RNN decoder : a conditional languate model to give the probability of the next character given the history and re-encoded feature grid\n","\n","W_1, and W_2 are learnded parameters of the model, and the softmax activatio function assigns probabilities over summation.\n","\n","\n"],"metadata":{"id":"2sXT--CouS6s"}},{"cell_type":"markdown","source":["## Dataset, DataLoader"],"metadata":{"id":"CL6QW0cArU0o"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","import os\n","import csv\n","from io import BytesIO\n","\n","import pickle\n","\n","\n","\"\"\"\n","IAM Dataset download\n","\"\"\"\n","\n","if not os.path.exists('/content/data/') :\n","    os.mkdir(\"/content/data\")\n","\n","splits = {'train': 'data/train.parquet', 'validation': 'data/validation.parquet', 'test': 'data/test.parquet'}\n","\n","\n","max_len = -1\n","str_len = -1\n","for iter in ['test', 'validation', 'train']:\n","\n","    if not os.path.exists(f'/content/data/{iter}') :\n","        os.mkdir(f\"/content/data/{iter}\")\n","    df = pd.read_parquet(\"hf://datasets/Teklia/IAM-line/\" + splits[iter])\n","    df_csv = df['text']\n","\n","    with open(os.path.join(\"/content/data\",f\"{iter}.csv\"), 'w', newline=\"\") as csvfile:\n","        spanwriter = csv.writer(csvfile, delimiter = '\\t', quotechar=\"|\")\n","        spanwriter.writerow([\"_path\",\"text\"])\n","        for id, txt in enumerate(df_csv):\n","            spanwriter.writerow([id,txt])\n","            if len(txt) > str_len:\n","                str_len =len(txt)\n","\n","    #df_csv.to_csv(path_or_buf=os.path.join('/content/data', f'{iter}.csv'), sep='\\t')\n","\n","    df_imag = df['image']\n","    for idx, img in enumerate(df_imag):\n","        image = Image.open( BytesIO(img['bytes']))\n","        #the max width : 2083, the fixed height : 64\n","        image=image.resize((2084,64))\n","        image.save(os.path.join(f\"./data/{iter}\",f\"{idx}.jpg\" ))\n","\n","        image = image.getchannel(0)\n","        width_size , height_size = image.size\n","        if(width_size > max_len):\n","            max_len =width_size\n","\n","print(max_len)\n","print(str_len)"],"metadata":{"id":"nQFQEHEjrXAV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#preprocessing\n","\n","We follow the image preprocessing steps of Puigcerver et al. [19, 46], which includes binarizing the images in a manner that preserves their original grayscale information [47], rescaling the images, and converting the images to JPEG format. Fig. 2 provides an example of a preprocessed image from each benchmark dataset.\n","\n","46: https://github.com/jpuigcerver/Laia, gitHub repository\n","\n","i findout it ..."],"metadata":{"id":"-x7KBHZw6o1K"}},{"cell_type":"code","source":["import skimage.color as img_color\n","from PIL import Image\n","import numpy as np\n","\n","def preprocess(image_path):\n","    image = Image.open(image_path)\n","    image = np.array(image)\n","    ## img to gray\n","    ## Actually aready it is ...\n","    if len(image) ==3:\n","      image = img_color.rgb2gray(image)\n","\n","    #normalize the image\n","    image = 1-image/255\n","\n","    return image"],"metadata":{"id":"EIpBK6nA6oM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from PIL import Image\n","import os\n","import numpy as np\n","import math\n","\n","class IAMDataset():\n","    def __init__(self, split, nclasses =None ,root_dir = \"/content/data\", transforms = None, target_transforms= None):\n","\n","        if  not split in [\"train\", \"validation\", 'test']:\n","          print(\"split must be among in ['train', 'validation','test']\")\n","          return\n","\n","        \"\"\"\n","          /content/data/train/\n","          /content/data/train/train.csv\n","\n","        \"\"\"\n","\n","        self.root_dir = root_dir\n","        self.image_dir = os.path.join(root_dir, split)\n","        self.csv_dir = os.path.join(root_dir,f\"{split}.csv\")\n","        self.transforms = transforms\n","        self.target_transforms = target_transforms\n","\n","\n","        label_data = pd.read_csv(os.path.join(self.csv_dir), delimiter=\"\\t\",quotechar=\"\\n\")\n","        label_data.columns = ['_path','text']\n","        self.img_labels = label_data\n","        self.label_len = [len(label.strip()) for label in label_data['text'] ]\n","        if  nclasses:\n","            self.nclasses = nclasses\n","\n","        else:\n","            char_set = set()\n","            cToInt = {}\n","            intToC = {}\n","\n","            for transc in self.img_labels['text']:\n","                char_set.update(transc)\n","\n","            char_set = list(char_set)\n","            char_set.sort()\n","            cdict = {c: (i+1) for i, c in enumerate(char_set)}\n","            icdict = {(i+1):c for i, c in enumerate(char_set)}\n","\n","            self.nclasses = {'classes':char_set,'c2i':cdict,'i2c':icdict}\n","\n","\n","        # create dictionaries for character to index and index to character\n","        # 0 index is reserved for CTC blank\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, index):\n","        img_path = os.path.join(self.image_dir, f'{self.img_labels.iloc[index,0]}.jpg')\n","        image = Image.open(img_path)\n","        img_np = np.array(image)\n","        image = torch.tensor(img_np)\n","        label = self.img_labels.iloc[index,1]\n","\n","        label_len = len(label.strip())\n","        #label_len = label_len[:np.newaxis]\n","\n","        \"\"\"\n","          actuaclly we only use the preprocessing channel 3 to 1 rgb2gray\n","        \"\"\"\n","        # 0 : padding , 1 : Go, 2: EOS, 3:UNK\n","        def preprocess_label(transc,tdecs):\n","\n","            word=[tdecs[c]+3 for c in transc.strip()]\n","            word.append(2)\n","            word[1:] = word\n","            word[0] = 1\n","\n","            # padd\n","            padd_size = int(math.floor(2084/3))\n","            act_len = len(word )\n","\n","            word = np.concatenate((word, np.zeros( padd_size- act_len) ))\n","            #[694]\n","            return word\n","\n","        def preprocess(image_path):\n","            image = Image.open(image_path)\n","            image = np.array(image)\n","            ## img to gray\n","            ## Actually aready it is ...\n","            if len(image) ==3:\n","              image = img_color.rgb2gray(image)\n","\n","            #normalize the image\n","            image = 1-image/255\n","\n","            return image\n","\n","\n","        if self.transforms:\n","          image = self.transforms(image)\n","\n","        image = preprocess(img_path)\n","        image = torch.tensor(image, dtype=torch.float32).reshape(1,64,2084)\n","        label = preprocess_label(label,self.nclasses['c2i'])\n","        label =torch.tensor(label,dtype=torch.float32)\n","\n","\n","        return image, label"],"metadata":{"id":"hq5J6ewhrt6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#NetStructure\n","\n","we fix the image heignt to 64 pixels while maintain aspect ratio, group images with similar widths, and pad with whitespace to facilitate batching\n","\n","CNN consists of seven convolutional layers,each followed by RELU, and them max-pooling layer,\n","\n","the 3, 5, 7 layer use batch normalization following the convolution in orderto speedup training\n","\n","Droupout is applied to the ouput of the seventh convolutional layer in\n","\n","Table 2: CNN specification.\n","Conv Pool\n","# filters Filter size Stride size Bn Pool size Stride size\n","64 (3,3) (1,1) (2,2) (2,2)\n","\n","128 (3,3) (1,1) (2,2) (2,2)\n","\n","256 (3,3) (1,1) X - -\n","\n","256 (3,3) (1,1) (2,1) (2,1)\n","\n","512 (3,3) (1,1) X - -\n","\n","512 (3,3) (1,1) (2,1) (2,1)\n","\n","512 (2,2) (1,1) X - -\n","\n","Stacked on the CNN is a single-layer, bidirectional long Short-Term Memory (BLSTM) encoder with 512 hidden units and a two-layer Gated Recurrent unit (GRU) decoder, each with 256 hidden units\n","The attentional decoder interprets the feature representation, focusing on the most important columns of re-encoded features\n","\n","\n","we train the networks for 200 epochs with a batch size of 8, stochastic gradient descent to learn the parameter weights, and the Adam optimizer to adapt the learning rate.\n","\n","\n"," As a regularization strategy, we implement ℓ\n","2\n","regularization loss and data augmentation by applying\n","random affine transformations to 20% of the training set images, including scaling, translating, rotating, and shearing\n","\n"," In addition, we employ gradient norm clipping and gradient\n","normalization in order to prevent exploding gradients."],"metadata":{"id":"iwOpfpiGEnjQ"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","\n","device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","\"\"\"\n","  Conv                                Pool\n","  filters Filter size Stride size Bn Pool size Stride size\n","    64 (3,3) (1,1)                    (2,2) (2,2)\n","\n","    128 (3,3) (1,1)                   (2,2) (2,2)\n","\n","    256 (3,3) (1,1)               O   (2,1) (2,1)\n","\n","    256 (3,3) (1,1)                   (2,1) (2,1)\n","\n","    512 (3,3) (1,1)               O   (2,1) (2,1)\n","\n","    512 (3,3) (1,1)                    - -\n","\n","    512 (2,2) (1,1)    \"valid\"    O    - -\n","    the size order is : (1, width)\n","\n","    the adjusted network ... came from the github ... Attention-OCR\n","\n","\"\"\"\n","\n","\n","class pCNN(nn.Module):\n","\n","    def __init__(self , number):\n","        super(pCNN,self).__init__()\n","\n","        if number in [3,5,7]:\n","            self.isBatch =True\n","        else :\n","            self.isBatch = False\n","        #paper say just a number of channel ... the last out\n","        filterNumber= [1,64,128,256,256,512,512,512]\n","\n","        inputs = filterNumber[number-1]\n","        outputs = filterNumber[number]\n","        filterSize = 3\n","        if(number ==7):\n","            filterSize = 2\n","\n","        pool_size = 2\n","        if (number >2):\n","            pool_size =1\n","\n","        padding = 'same'\n","        if(number == 7):\n","            padding = 'valid'\n","\n","        self.MaxPool =True\n","        if (number==6 or number ==7):\n","            self.MaxPool =False\n","\n","        self.ConvLayer = nn.Conv2d(inputs, outputs,filterSize, padding=padding)\n","\n","        self.ReLULayer = nn.ReLU()\n","        if(self.isBatch) :\n","            self.BatchLayer = nn.BatchNorm2d(outputs)\n","\n","        if(self.MaxPool):\n","            self.nextLayer = nn.MaxPool2d((2,pool_size), stride = (2,pool_size))\n","\n","    def forward(self, x):\n","        x= self.ConvLayer(x)\n","        logits= self.ReLULayer(x)\n","        if self.isBatch:\n","          logits=self.BatchLayer(logits)\n","        if self.MaxPool:\n","          logits= self.nextLayer(logits)\n","\n","        return logits\n","\n","class CNNnet(nn.Module):\n","    def __init__(self):\n","        super(CNNnet,self).__init__()\n","        self.conv1 = pCNN(1)\n","        self.conv2 = pCNN(2)\n","        self.conv3 = pCNN(3)\n","        self.conv4 = pCNN(4)\n","        self.conv5 = pCNN(5)\n","        self.conv6 = pCNN(6)\n","        self.conv7 = pCNN(7)\n","        self.Droplayer = nn.Dropout()\n","\n","    def forward(self,x):\n","\n","        #[batch, 64, 2520]\n","        y = self.conv1(x)\n","        y = self.conv2(y)\n","        y = self.conv3(y)\n","        y = self.conv4(y)\n","        y = self.conv5(y)\n","        y = self.conv6(y)\n","        y = self.conv7(y)\n","        logits = self.Droplayer(y)\n","        logits=logits.squeeze()\n","        return logits\n"],"metadata":{"collapsed":true,"id":"ReZhKPqdGeZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Construct RNN - encoder// RNN - decoder\n","\n","RNN - encoder\n","\n","slides across each row of V, and at each time-step t, recursively updates a hidden state h_t, using v_t in V as input:\n","\n","  h_t = f(v_t, h_t-1; seta)\n","  "],"metadata":{"id":"mRNeaX1v1YMi"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class CNN_RNNEncoder(nn.Module):\n","    def __init__(self, num_hidden=256):\n","        super(CNN_RNNEncoder,self).__init__()\n","        \"\"\"\n","            the part of from image process to just before of attention ...\n","        \"\"\"\n","\n","        self.CNNLayer = CNNnet()\n","        #[batch,channel,width]\n","        #[8, 256,541]\n","        self.LSTMLayer = nn.LSTM(512, num_hidden, 1, bidirectional=True)\n","\n","    def forward(self,x):\n","        logits = self.CNNLayer(x)\n","        #[batch,channel, width]\n","\n","        logits = logits.permute(2,0,1)\n","        #[width, batch, channel]\n","\n","        width_size, batch_size, channel_size = logits.size()\n","\n","        pre_encoder, (hidden_state, __)  = self.LSTMLayer(logits)\n","        #[width,batch,channel] , (2,batch,2*num_hidden)\n","\n","        hidden_state = torch.concat([hidden_state[0],hidden_state[1]], dim=1)\n","\n","        pre_encoder = pre_encoder.transpose(0,1)\n","\n","        return pre_encoder, hidden_state\n"],"metadata":{"id":"BO4gaygq4Twh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RNNDecoder(nn.Module):\n","    def __init__(self,num_hidden=256, nclasses=82):\n","        super(RNNDecoder,self).__init__()\n","        \"\"\"\n","            attention and decoder\n","            we given the target_weight, target_input, decoder_input\n","            all word : 79 + padd, go, stop, UNK => 82\n","\n","            attention_vec_size  < - attn_size  : 512\n","            attn_length = attention_state[1]\n","            attn_size = attenion_state[2] : 512\n","        \"\"\"\n","\n","        \"\"\"\n","            First, we run the cell on a combination of the input and previous attention masks:\n","            cell_output, new_state = cell(linear(input, prev_attn), prev_state).\n","            Then, we calculate new attention masks:\n","            new_attn = softmax(V^T * tanh(W * attention_states + U * new_state))\n","\n","            and then we calculate the output:\n","            output = linear(cell_output, new_attn).\n","            state: The state of each decoder cell the final time-step.\n","            It is a 2D Tensor of shape [batch_size x cell.state_size].\n","        \"\"\"\n","        \"\"\"\n","            decoder_inputs: A list of 1D batch-sized int32 Tensors (decoder inputs).\n","        \"\"\"\n","\n","        self.num_hidden = num_hidden\n","        # AttentionLayer\n","        self.attentionMat = nn.Linear(512,512)\n","\n","        # it is a weigthed sum of query ...\n","        self.encoder_attention= nn.Conv2d(512,512,(1,1),bias=False,padding='same')\n","        self.attentionVec = nn.Linear(512,512, bias = False)\n","\n","        self.EmbeddingToSymbol = nn.Linear(512,nclasses)\n","        self.EmbeddingLookup = nn.Embedding(nclasses, 512 , padding_idx=0)\n","\n","        self.CombinationInputAttention = nn.Linear(512,128)\n","\n","        #GRU bidirection , 2layer\n","        self.GRULayer_first = nn.GRUCell(128,512)\n","        self.GRULayer_second = nn.GRUCell(512,512)\n","\n","        self.OutputProjection = nn.Linear(512,512)\n","\n","    def forward(self, encoder_inputs, hidden_states):\n","\n","        # the label length : 694\n","\n","        # bucket[0] = 2084/4 = 541, bucket[1]= 2084/3 => 693\n","        # there is no bucket ... no resize, or permutate\n","        # attention_states = encoder_inputs, initial_state = hidden_states\n","        # [batch ,width,channel] ,[batch,channel]\n","\n","        ## do embedding_attention_decoder !!\n","        # To calculate W1 * h_t we use a 1- by- 1 convolution, reen to reshape before\n","        # encoder_inputs < = attention_state, label_inputs <= decoder_inputs\n","\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","\n","        batch_size, attn_length, attn_size = encoder_inputs.size()\n","\n","        hidden = encoder_inputs\n","\n","        def attention(query,hidden):\n","            #query = hidden_states, state\n","            #[batch, 512]\n","            batch_size, attn_size = query.size()\n","\n","            hidden = hidden.permute(0,2,1)\n","            #[batch, attn_length, attn_size] => [batch,attn_size,attn_length]\n","            hidden = torch.reshape(hidden,[-1,attn_size,1,attn_length])\n","            #[batch, attn_length, attn_size] => [batch,attn_size, 1,attn_length ]\n","\n","            y = self.attentionMat(query)\n","            #[batch,attn_size]\n","\n","            y = torch.reshape(y,[-1,1,1,attn_size])\n","\n","            #Attention mask is a softmax of v^T * tanh(...)\n","            hidden_features = self.encoder_attention(hidden)\n","            #hidden :  [batch, atten_size,1,attn_length]\n","            #hidden_features = [batch, atten_size,1,attn_length]\n","\n","            hidden_features = hidden_features.permute(0,3,2,1)\n","            #[batch,attn_length,1,attn_size]\n","\n","            s = torch.tanh(hidden_features +y)\n","            #[batch,attn_length,1,attn_size] + [batch,1,1,attn_size]\n","            # = > [batch,attn_length,1,attn_size]\n","\n","            s = self.attentionVec(s)\n","            s = torch.sum(s, (2,3))                         # theta_ij\n","            #[batch,attn_length]\n","\n","            ss = torch.softmax(s,dim=1)\n","            #[batch,attn_length]\n","\n","            # the attention-weighted vector ds\n","            ds = torch.sum(torch.reshape(ss,[-1,attn_length,1,1])* hidden.transpose(1,3), [1,2])\n","            ds = torch.reshape(ds,[-1,attn_size])\n","            #[batch,attn_size]\n","\n","            return ds, ss\n","\n","        def loop_function(prev):\n","            \"\"\"\n","                embedding and argmax ...\n","            \"\"\"\n","            prev = self.EmbeddingToSymbol(prev)\n","            #[batch,attn_size] = > [batch, symbol]\n","\n","            prev_symbol = prev.argmax(1).squeeze()\n","\n","            emb_prev = self.EmbeddingLookup(prev_symbol)\n","            #[batch,attn_size]\n","            return emb_prev\n","\n","        def GRULayer(input,state):\n","            \"\"\"\n","              GRUCell return => new_h, new_h // cell_state and hidden_state are same\n","              No way to bidirection on the github\n","            \"\"\"\n","\n","            input = self.GRULayer_first(input,state)\n","            result = self.GRULayer_second(input,state)\n","            #[batch, 512]\n","\n","            return result\n","\n","        outputs = []\n","        #[694,batch, 512]\n","        attention_weights_history = []\n","        #[694,batch,512]\n","\n","        state = hidden_states\n","\n","        ## attn, attn_weights <- came from img  [batch, attn_size]\n","        ## inp <- decoder_inputs <- label [batch, 694] ?\n","\n","        # the length of label decoder_inputs :  694\n","\n","        prev =None\n","        attns = torch.zeros(batch_size,attn_size).to(device)\n","\n","        for idx in range(694):\n","            if prev ==None:\n","                inp = torch.ones(batch_size,dtype=int).to(device)\n","                inp = self.EmbeddingLookup(inp)\n","\n","            else :\n","                inp = loop_function(prev)\n","                #[batch,512]\n","\n","            x = self.CombinationInputAttention(inp + attns)\n","            #[batch,512]\n","            state = GRULayer(x, state)\n","\n","\n","            attns, attn_weights = attention(state, hidden)\n","            attention_weights_history.append(attn_weights)\n","\n","            output = torch.matmul(state+attns, self.params.get('OutputProjection'))\n","            output = output+ self.params.get('OutputProjectionVec')\n","\n","            prev = output\n","            outputs.append(output)\n","\n","        outputs = torch.stack(outputs, dim =0)\n","        #[attn_length, batch, attn_size]\n","        attention_weights_history = torch.stack(attention_weights_history, dim=0)\n","        embedd_outputs = self.EmbeddingToSymbol(outputs)\n","        #[attn_length, batch, symbol]\n","\n","        return embedd_outputs ,state, attention_weights_history\n","\n"],"metadata":{"id":"59yr1Sr68eQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class HTRnet(nn.Module):\n","  def __init__(self,nclasses):\n","      super(HTRnet,self).__init__()\n","      self.EncoderLayer = CNN_RNNEncoder(256)\n","\n","      self.DecoderLayer = RNNDecoder(256,nclasses=nclasses)\n","\n","  def forward(self,x):\n","\n","      pre_encoder_inputs,hidden_state = self.EncoderLayer(x)\n","      outputs, states, attention_weights_history= self.DecoderLayer(pre_encoder_inputs, hidden_state)\n","\n","      return  outputs, states, attention_weights_history\n","\n"],"metadata":{"id":"3Z5Twy0ECKxq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Metrix\n","\n","define the loss, optimizer and score function"],"metadata":{"id":"G4g79ZWVCHci"}},{"cell_type":"code","source":["!pip install editdistance"],"metadata":{"id":"7LlLH3IulZqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#only we use the char dict\n","\n","import editdistance\n","\n","class CER :\n","    def __init__(self):\n","        self.total_dist = 0\n","        self.total_len =1\n","\n","    def update(self, prediction, target):\n","        dist = float(editdistance.eval(prediction,target))\n","        self.total_dist += dist\n","        self.total_len += len(target)\n","\n","    def score(self):\n","        return self.total_dist / self.total_len\n","\n","    def reset(self):\n","        self.total_dist = 0\n","        self.total_len = 1\n"],"metadata":{"id":"35nlurGpCPdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Trainer, Eval\n","\n","\n","about : diffent length sequence CrossEntropy\n","\n","\n","https://danijar.com/variable-sequence-lengths-in-tensorflow/"],"metadata":{"id":"75caG4-xDL-X"}},{"cell_type":"code","source":["import tqdm\n","from torchvision.transforms import v2\n","\n","class HTRTrainer(nn.Module):\n","    def __init__(self):\n","        super(HTRTrainer, self).__init__()\n","\n","        self.prepare_dataloaders()\n","        self.prepare_net()\n","        self.prepare_losses()\n","        self.prepare_optimizers()\n","\n","\n","    def prepare_dataloaders(self):\n","\n","        transforms = v2.Compose([\n","            v2.ToImage(),\n","            v2.ToDtype(torch.float32,scale =True),\n","            v2.RandomAffine(degrees = (-45,45),scale = (80,120),shear = ([-16,16,-16,16])),\n","            v2.ToDtype(torch.uint8,scale =True),\n","        ])\n","\n","        train_set = IAMDataset( split = \"train\",transforms=transforms)\n","        self.classes = train_set.nclasses\n","\n","        # save classes in data folder\n","        #np.save(os.path.join(\"/content/data\", 'classes.npy'), self.classes)\n","\n","        # create dictionaries for character to index and index to character\n","        # 0 index is reserved for pad blank_index\n","\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","\n","        val_set = IAMDataset( split = \"validation\",nclasses = self.classes)\n","        test_set = IAMDataset( split = \"test\", nclasses= self.classes)\n","\n","\n","        \"\"\"\n","        num_epoch : 200\n","        batch_size : 8\n","        \"\"\"\n","\n","        train_loader = DataLoader(train_set, batch_size=8,shuffle=True)\n","        test_loader = DataLoader(test_set, batch_size=8,shuffle=True)\n","        val_loader = DataLoader(val_set, batch_size=8,shuffle=True)\n","\n","        self.loaders ={\"train\": train_loader,\"val\": val_loader,\"test\": test_loader}\n","\n","    def prepare_net(self):\n","        classes = self.classes['classes']\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","        self.net = HTRnet(nclasses=len(classes)+4).to(device)   # 83 : 0~ 82\n","\n","    def prepare_losses(self):\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","        self.CrossLayer = nn.CrossEntropyLoss(ignore_index=0,reduction=\"mean\").to(device)\n","\n","    def prepare_optimizers(self):\n","\n","        optimizer = torch.optim.Adam(self.net.parameters())\n","        self.optimizer = optimizer\n","\n","    def decode(self, tdec, tdict,blank_id = 0, start_id = 1, end_id = 2):\n","        vec = []\n","        for c in tdec.tolist() :\n","            if c > 3:\n","                vec.append(tdict[c-3])\n","            elif c == end_id:\n","                break\n","\n","        # there is a subtle thing that i didn't know ... from the decoder there is a different between Go, padd_id ?!?\n","        vec = ''.join(vec).strip()\n","        return vec\n","\n","    def sample_decoding(self):\n","\n","        # get a random image from the test set\n","        img, transcr = self.loaders['val'].dataset[np.random.randint(0, len(self.loaders['val'].dataset))]\n","\n","        self.net.eval()\n","        with torch.no_grad():\n","            test_object = self.net(img)\n","            test_object = test_object[0]\n","\n","        self.net.train()\n","\n","        tdec = test_object.argmax(2).permute(1, 0).cpu().numpy().squeeze()\n","\n","\n","        dec_transcr = self.decode(tdec, self.classes['i2c'])\n","\n","        print('orig:: ' + transcr.strip())\n","        print('pred:: ' + dec_transcr.strip())\n","\n","    def train(self, epoch):\n","\n","        self.net.train()\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","        t = tqdm.tqdm(self.loaders['train'])\n","        t.set_description('Epoch {}'.format(epoch))\n","\n","        max_grad_norm = 5.0\n","        for iter_idx, (img, transcr) in enumerate(t):\n","\n","            self.optimizer.zero_grad()\n","            img = img.to(device)\n","            #transcr.to(device)\n","\n","            outputs, states, attention_weights_history = self.net(img)\n","            #[attn_length,batch, symbol]    attn_length = legnth of transcr\n","\n","            dec_transcr= outputs.permute(1,2,0).to('cpu')\n","            #transcr = transcr.transpose(0,1)\n","            #[batch, attn_length,sybol]\n","            transcr = transcr.long()\n","            loss = self.CrossLayer(dec_transcr, transcr)\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(self.net.parameters(),max_grad_norm )\n","            self.optimizer.step()\n","\n","    def test(self, epoch, tset='test'):\n","\n","        self.net.eval()\n","        device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n","        self.net.to(device)\n","        if tset=='test':\n","            loader = self.loaders['test']\n","        elif tset=='val':\n","            loader = self.loaders['val']\n","        else:\n","            print(\"not recognized set in test function\")\n","\n","        print('####################### Evaluating {} set at epoch {} #######################'.format(tset, epoch))\n","\n","        cer = CER()\n","        for (imgs, transcrs) in tqdm.tqdm(loader):\n","            imgs = imgs.to(device)\n","\n","            #transcrs = transcrs.to(device)\n","            with torch.no_grad():\n","                o, state, attention_weights_history = self.net(imgs)\n","\n","            #[batch,694,80]\n","            tdecs = o.transpose(0,1).argmax(2).cpu().numpy().squeeze()\n","\n","            for tdec, transcr in zip(tdecs, transcrs):\n","                transcr = self.decode(transcr, self.classes['i2c'])\n","                dec_transcr = self.decode(tdec, self.classes['i2c'])\n","\n","                cer.update(dec_transcr, transcr)\n","\n","        cer_score = cer.score()\n","\n","        print('CER at epoch {}: {:.3f}'.format(epoch, cer_score))\n","\n","\n","    def save(self, epoch):\n","        print('####################### Saving model at epoch {} #######################'.format(epoch))\n","        if not os.path.exists('./saved_models'):\n","            os.makedirs('saved_models')\n","\n","        torch.save(self.net.cpu().state_dict(), './saved_models/htrnet_{}.pt'.format(epoch))\n"],"metadata":{"id":"2m36Vht6DUj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","htr_trainer = HTRTrainer()\n","\n","max_epochs =75\n","\n","print('Training Started!')\n","\n","htr_trainer.test(0)\n","\n","for epoch in range(1, max_epochs + 1):\n","    htr_trainer.train(epoch)\n","\n","    # save and evaluate the current model\n","    if epoch % 5 == 0:\n","        htr_trainer.save(epoch)\n","        htr_trainer.test(epoch, 'test')\n","        htr_trainer.test(epoch, 'val')\n","\n","    # save the  model\n","    if epoch % 25 == 0:\n","        if not os.path.exists('./saved_models'):\n","            os.makedirs('./saved_models')\n","        file_name= time.strftime('%Y_%m_%d', time.localtime(time .time()))\n","        torch.save(htr_trainer.net.cpu().state_dict(), f'./saved_models/{file_name}_train.np')\n","\n","while(True):\n","    time.sleep(1000)\n","    print(\"hello\")"],"metadata":{"id":"S52hNfDIo3pu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["htr_trainer.to(device)\n","for epoch in range(1, max_epochs + 1):\n","    htr_trainer.train(epoch)\n","\n","    # save and evaluate the current model\n","    if epoch % 5 == 0:\n","        htr_trainer.save(epoch)\n","        htr_trainer.test(epoch, 'test')\n","        htr_trainer.test(epoch, 'val')\n","\n","    # save the  model\n","    if epoch % 25 == 0:\n","        if not os.path.exists('./saved_models'):\n","            os.makedirs('./saved_models')\n","        file_name= time.strftime('%Y_%m_%d', time.localtime(time .time()))\n","        torch.save(htr_trainer.net.state_dict(), f'./saved_models/{file_name}_train.np')\n","\n","while(True):\n","    time.sleep(1000)\n","    print(\"hello\")"],"metadata":{"id":"3DiYYg2uq8eW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Encoder-decoder models and attention\n","\n","url:=https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV\n"],"metadata":{"id":"nohMRruRN42Q"}}]}