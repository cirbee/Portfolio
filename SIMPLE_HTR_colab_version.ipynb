{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM9p0q+EYXFXiPDzx6cSre/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# SIMPLE/HTR <- colab version"],"metadata":{"id":"BFuRfy0kx9Gy"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","import pandas as pd\n","from PIL import Image\n","import numpy as np\n","import os\n","import csv\n","from io import BytesIO\n","\n","import pickle\n","\n","\n","\"\"\"\n","IAM Dataset download\n","\"\"\"\n","\n","if not os.path.exists('/content/data/') :\n","    os.mkdir(\"/content/data\")\n","\n","splits = {'train': 'data/train.parquet', 'validation': 'data/validation.parquet', 'test': 'data/test.parquet'}\n","\n","\n","max_len = -1\n","str_len = -1\n","for iter in ['test', 'validation', 'train']:\n","\n","    if not os.path.exists(f'/content/data/{iter}') :\n","        os.mkdir(f\"/content/data/{iter}\")\n","    df = pd.read_parquet(\"hf://datasets/Teklia/IAM-line/\" + splits[iter])\n","    df_csv = df['text']\n","\n","    with open(os.path.join(\"/content/data\",f\"{iter}.csv\"), 'w', newline=\"\") as csvfile:\n","        spanwriter = csv.writer(csvfile, delimiter = '\\t', quotechar=\"|\")\n","        spanwriter.writerow([\"_path\",\"text\"])\n","        for id, txt in enumerate(df_csv):\n","            spanwriter.writerow([id,txt])\n","            if len(txt) > str_len:\n","                str_len =len(txt)\n","\n","    #df_csv.to_csv(path_or_buf=os.path.join('/content/data', f'{iter}.csv'), sep='\\t')\n","\n","    df_imag = df['image']\n","    for idx, img in enumerate(df_imag):\n","        image = Image.open( BytesIO(img['bytes']))\n","\n","        image.save(os.path.join(f\"./data/{iter}\",f\"{idx}.jpg\" ))\n","\n","        width_size , height_size = image.size\n","        if(width_size > max_len):\n","            max_len =width_size\n","\n","        if(height_size != 128):\n","            print(height_size)\n","\n","print(max_len)\n","print(str_len)"],"metadata":{"id":"iKuaE89cyDiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#pre-processing\n","retain aspect ratio of images and use batches of padded images in order to effectively use mini-batch Stochastic Gradient Descent(SGD)\n"],"metadata":{"id":"J0YcuXaFyXi-"}},{"cell_type":"code","source":["# All images are resized to a resulution of 128x1024 pixels for line images or 64x256 pixels.\n","# Initial images are padded in order to attain the aformentioned fixed size.\n","\n","# IAM dataset image have a size width x 128(height), the max width size = 5027\n","import torch\n","import math\n","from torch import nn\n","from PIL import Image\n","import numpy as np\n","from torchvision.transforms import functional as F\n","\n","# target_size : 1024 x 128\n","def preprocess(image) :\n","    max_height = 128\n","    max_width = 1024\n","    #image = image.convert(\"L\")\n","\n","    width_size, height_size = image.size\n","    if (height_size > 128):\n","        print(width_size, height_size)\n","        ratio = 128/height_size\n","        #image = image.resize((math.ceil(width_size*ratio), 128))\n","        image = F.resize(image , ( 128,math.ceil(width_size*ratio)))\n","        width_size, height_size = image.size\n","\n","    if (width_size > 1024):\n","        ratio = 1024/width_size\n","        #image = image.resize( (1024, math.ceil(height_size *ratio) ) )\n","        image =F.resize(image , (math.ceil(height_size *ratio) ,1024))\n","    image = np.array(image)\n","    image = torch.tensor(image)\n","    image = padd_img(image)\n","\n","    return image\n","\n","\n","    # padd it 1024x128\n","def padd_img(img):\n","    height_tar, width_tar = 128,1024\n","    height_size, width_size = img.shape\n","\n","    ## 128 - 51 = 77\n","    ## 77/2 = > 38  76 + 51 =127\n","\n","    left = math.floor((width_tar - width_size)/2)\n","    right =math.floor((width_tar -width_size)/2)\n","    if(width_size %2 == 1):\n","        right = math.floor((width_tar - width_size)/2) +1\n","    top = math.floor((height_tar - height_size)/2)\n","    bottom = math.floor((height_tar - height_size)/2)\n","    if (height_size %2 ==1):\n","        bottom = math.floor((height_tar - height_size)/2) + 1\n","\n","    img = nn.functional.pad(img,(left,right,top,bottom),mode=\"constant\", value=255)\n","    return img\n","\n","\n","#During traing, image augmentation is performed.\n","#considering only rotation and skew of small magnitude in order to generate valid images\n","#Additionaly, gaussian nosie is added to the images\n","from torchvision.transforms import v2\n","\"\"\"\n","transforms = v2.RandomApply(\n","    torch.nn.ModuleList([\n","        v2.RandomAffine(degrees=(-10,10),translate=(0.01,0.05), scale = (0.8,1.2),shear = (-10,10))\n","        ]) , p=0.5 )\n","\"\"\"\n","transforms = v2.RandomApply(\n","    torch.nn.ModuleList([\n","        v2.RandomAffine(degrees = (0,0),translate=(0.01,0.05), scale = (0.8,1),shear = (-10,10),fill=255)\n","        ]) , p=0.5 )\n","\n","# rotation image ... are cutted so did;nt show the character in image ...\n","# even i didn't see the chacracter it is... not good\n","# rotation, translation, scaling and shearing  and gray-scale erosion and dilation\n","# the paper named \"Are Multidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\" said it toolkit defualt value but didn't found it\n","\n","\"\"\"\n","We perform adequate random distortions on the input images,\n","in order to artificially augment the training samples and reduce overfitting.\n","\n","These distortions include: rotation,translation, scaling and shearing\n","(all performed as a single affine transform) and gray-scale erosion and dilation.\n","Each of these operations is applied dynamically and independently on each image of the training batch (each with 0.5 probability).\n","Thus, the exact same image is virtually never observed twice\n","during training.\n","The parameters controlling each distortion (e.g. rotation angle, scaling factor, erosion kernel, etc.) are sampled from a fixed distribution.\n","    -- Are Multidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\n","\"\"\"\n","# transforms  => preprocessing\n","\n","\n","#Each word/line transcription has spaces added before and after, \"He rose from\" => \" He rose from \"\n","\n","def pre_processing_target(label):\n","    return \" \" + label + \" \"\n","target_transforms = pre_processing_target\n","\n","#This operation aims to assist the system during the training phase, For the testing phase, these additional spaces are discarded\n"],"metadata":{"id":"C9BQXO6-ztlL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torchvision.transforms import functional as F\n","import os\n","from PIL import Image\n","import pandas as pd\n","\n","class IAMData():\n","    def __init__(self,split ,root_path = '/content/data' , transform =None,target_transform = None ):\n","\n","        assert os.path.exists(root_path), \"valueError: IAMData 'wrong root_path' \"\n","\n","        assert  split in ['train', 'test', 'validation'] , \"valueError: IAMData 'split must be one of ['train', 'test', 'validation]'\"\n","\n","        self.preprocess = None\n","        if split == 'train':\n","            self.preprocess = preprocess\n","            char_set = set()\n","\n","\n","        self.annotations_file = os.path.join(root_path, f\"{split}.csv\")\n","        self.img_dir = os.path.join(root_path, split)\n","\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","        self.img_labels = pd.read_csv(self.annotations_file, delimiter = '\\t', quotechar='|')\n","\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self,idx):\n","        img_path = os.path.join(self.img_dir, f\"{self.img_labels.iloc[idx,0]}.jpg\")\n","\n","        image = Image.open(img_path)\n","        label =self.img_labels.iloc[idx,1]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","\n","        if self.preprocess :\n","            image = self.preprocess(image)\n","\n","        return image, label\n"],"metadata":{"id":"UhRrwJ_g6DaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ds = IAMData('test')\n","print(test_ds[3])"],"metadata":{"id":"fgEl_9l7ChOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dst = IAMData('train', transform= transforms , target_transform= target_transforms)\n","print(train_dst[9])\n","\n"],"metadata":{"id":"98B1cSQ5EvUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.transforms.functional import to_pil_image\n","import matplotlib.pyplot as plt\n","\n","for i in range(5):\n","    img, label = train_dst[i]\n","\n","    plt.figure()\n","    plt.imshow(to_pil_image(img), cmap = 'gray')\n","    print(label)\n","    print(img.shape)"],"metadata":{"id":"zVYM__rFGlH-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#architectural\n","replace the column-wise concatenation step between the CNN backbone and the recurrent head with a max-pooling step. such a choice not only redueces the required parameters but has an intuitive motivation: we care only about the existence of a character and not its vertical poision"],"metadata":{"id":"osjkL5RhymeK"}},{"cell_type":"markdown","source":["#Convolutional Backbone"],"metadata":{"id":"vCetWnOzPvwb"}},{"cell_type":"code","source":["\"\"\"\n","In our model, the convolutional backbone is made\n","up of standard convolutional layers and ResNet blocks [12], interspersed with\n","max-pooling and dropout layers.\n","\"\"\"\n","\n","\"\"\"class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\"\"\"\n","import torch\n","from torch.nn import functional as F\n","\n","class Residual_Block(nn.Module):\n","    def __init__(self,in_channels, out_channels,stride =1):\n","        super(Residual_Block,self).__init__()\n","\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_channels != out_channels:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","\n","        return out\n","\n","class ConvolutionalBackbone(nn.Module):\n","    def __init__(self):\n","        super(ConvolutionalBackbone, self).__init__()\n","\n","        self.CNNLayer = nn.Conv2d(1,32,(7,7), stride=1 , padding=3)\n","\n","        self.MaxPoolFirst = nn.MaxPool2d((2,2))\n","        self.ResBlockFirst = nn.Sequential(\n","              Residual_Block(32,64),\n","              Residual_Block(64,64)\n","        )\n","\n","        self.MaxPoolSecond = nn.MaxPool2d((2,2))\n","        self.ResBlockSecond = nn.Sequential(\n","              Residual_Block(64,128),\n","              Residual_Block(128,128),\n","              Residual_Block(128,128),\n","              Residual_Block(128,128)\n","        )\n","\n","        self.MaxPoolThird = nn.MaxPool2d((2,2))\n","        self.ResBlockThird = nn.Sequential(\n","              Residual_Block(128,256),\n","              Residual_Block(256,256),\n","              Residual_Block(256,256),\n","              Residual_Block(256,256)\n","        )\n","\n","    def forward(self,x):\n","        x = x.reshape([-1,1,128,1024])\n","        #[128,1024]\n","        logits = self.CNNLayer(x)\n","        logits = self.MaxPoolFirst(logits)\n","        logits = self.ResBlockFirst(logits)\n","\n","        logits = self.MaxPoolSecond(logits)\n","        logits = self.ResBlockSecond(logits)\n","\n","        logits = self.MaxPoolThird(logits)\n","        logits = self.ResBlockThird(logits)\n","\n","        #[batch, channel, height, width ] =[batch, 256,16,128]\n","        return logits\n","\n","\n","\n","\n","# the first layer is a 7x7 convolution with 32 output channels\n","\n","\n","#"],"metadata":{"id":"l-hl2HulO5qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = ConvolutionalBackbone()\n","\n","test_ds = IAMData('test')\n","a = torch.rand(2,1,128,1024)\n","a = model(a)\n","print(a.shape)"],"metadata":{"id":"Cdz6ZSzecNmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Flattening Operation => ColumneWiseMaxPooling"],"metadata":{"id":"Rl1GM3mUPzb0"}},{"cell_type":"code","source":["\"\"\"\n","    The convolutional backbone output should be transformed into a sequence of features in order to processed by recurrent networks.\n","    Typical HTR approaches, assume a column-wise approach (towards the writing direction) to ideally simulate a character by character processing.\n","    the CNN output is flattened by a max-pooling operation in a column-wise manner\n","\n","    ... Apart from the apparent computational advantage,\n","    column-wise max-pooling achieves model translation invariance in the vertical direction\n","\"\"\"\n","\n","#.. it is too small, it is better to apply it just function ... in whole class ...\n","class ColumnwiseMaxPool(nn.Module):\n","    def __init__(self):\n","        super(ColumnwiseMaxPool,self).__init__()\n","\n","    def forward(self,CNNoutput):\n","        MaxPoolData  =torch.max(CNNoutput, dim = 2)\n","        #[batch,channel,width]\n","        return MaxPoolData"],"metadata":{"id":"opKVmzfqP2L6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Recurrent Head\n"],"metadata":{"id":"YOQMqwPyP2r9"}},{"cell_type":"code","source":["\"\"\"\n","    The recurrent component consists of 3 stacked Bidirectional Long Short-Term Memory (BiLSTM) units of hidden size 256. These are followed\n","by a linear projection layer, which converts the sequence to a size equal to the number of possible character tokens, nclasses (including the blank character, required by CTC).\n","\n","    The final output of the recurrent part can be translated into a sequence of probability distributions by applying a softmax operation.\n","\"\"\"\n","\n","\"\"\"\n"," During evaluation, the aforementioned greedy decoding is performed by selecting the character\n","  with the highest probability at each step and then removing the blank characters from the resulting sequence [8].\n","\"\"\"\n","\n","class RecurrentHead(nn.Module):\n","    def __init__(self, nclasses):\n","        super(RecurrentHead,self).__init__()\n","        self.BiLSTM = nn.LSTM(256,256,num_layers=3,bidirectional=True)\n","        self.Projection = nn.Linear(512,nclasses)\n","        #nclasses include the blank character\n","\n","    def forward(self,x):\n","        x = x.permute([2,0,1])\n","        #[batch,channel, width] = > [width,batch,channel]\n","        outputs, (hidden , cell) = self.BiLSTM(x)\n","        logits = self.Projection(outputs)\n","        #[width,batch,2 * channel] => [width,batch, nclasses ]\n","        logits = logits.transpose(0,1)\n","        #[batch,width,nclasses]\n","\n","        return logits\n","\n"],"metadata":{"id":"H7yqXOaIP4WM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CTC shortcut\n"],"metadata":{"id":"4dzfCOmZoHjL"}},{"cell_type":"code","source":["\"\"\"\n","Architecture-wise, the CTC shortcut module consists only of\n","a single 1D convolutional layer, with kernel size 3. Its output channels equal\n","to the number of the possible character tokens (nclasses). Therefore, the 1D\n","convolutional layer is responsible for straightforwardly encoding context-wise\n","information and providing an alternative decoding path.\n","\"\"\"\n","\n","class CTCshortcut(nn.Module):\n","    def __init__(self, nclasses):\n","        # nclasses include the black_id\n","        super(CTCshortcut,self).__init__()\n","\n","        self.ConLayer = nn.Conv1d(128,nclasses,3,padding = 1)\n","\n","\n","    def forward(self,x):\n","        logits = self.ConLayer(x)\n","        #[batch,channel,width]=>[batch,nclasses,width]\n","        logits = logits.transpose(1,2)\n","        #[batch,nclasses,width] => [batch,width,nclasses]\n","\n","        return logits\n"],"metadata":{"id":"VPkqAdFWoJn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#CTC Loss\n","\n","the multi-task loss is written as\n","\n","$L_{CTC}(f_{rec}(f_{cnn}(I)) ;s ) +0.1 L_{CTC}((f_{shortcut}(f_{cnn}(I)); s))$\n","\n","\n","\"\"\"\n","\n","    The CTC shortcut is trained along with the main architecture using\n","    a multitask loss by adding the corresponding CTC losses of the two branches with the appropriate weights\n","\n","    Since CTC shortcut acts only as an auxiliary training path, it is weighted by 0.1\n","    to reduce its relative contribution to the overall loss.\n","\n","\"\"\"\n"],"metadata":{"id":"CCQqKWCDs96E"}},{"cell_type":"code","source":["#total HTRnet\n","\n","class HTRnet(nn.Module):\n","    def __init__(self, nclasses):\n","        #nclasses include the blank_id\n","        super(HTRnet,self).__init__()\n","\n","        self.CNNBacbone = ConvolutionalBackbone()\n","        # columnewise Maxpooling\n","\n","        self.RecHead = RecurrentHead(nclasses)\n","        self.CTCshort = CTCshortcut(nclasses)\n","\n","\n","    def forward(self, x):\n","        features = self.CNNBackbone(x)\n","        #[batch,channel,height, width]\n","\n","        # columnewise Maxpooling\n","        colMaxPool = torch.max(features,dim =2)\n","        #[batch,channel, width]\n","\n","        RecOutput =  self.RecHead(colMaxPool)\n","        CTCOutput = self.CTCshort(colMaxPool)\n","\n","        return RecOutput, CTCOutput\n"],"metadata":{"id":"oOz18xAYxJWK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = HTRnet(93)\n","print(model)"],"metadata":{"collapsed":true,"id":"tAApuuwIz23D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training\n","add and extra shortcut branch, consisting of a single 1D convoluion layer, at the ouptput of the CNN backbone. this branch results to a an extra character sequnce estimation, trained in parallel to the recurrent branch. both branches use a CTC loss. the motivation behind such a choice comes from the increased difficulty of training recurrent layers. However, if such a strainghtforward shortcut exists, the output of the CNN backbone should coverge to more discriminative features, ideal for fully harnessing the power of recurrent layers compared to an end-to-end training scheme"],"metadata":{"id":"BeotOKjUy6rV"}},{"cell_type":"code","source":["class HTRTrainer(nn.Module):\n","    def __init__(self):\n","        super(HTRTrainer,self).__init__()\n","\n","        self.prepare_dataloaders()\n","        self.prepare_net()\n","        self.prepare_losses()\n","        self.prepare_optimizers()\n","\n","    def prepare_dataloaders(self):\n","        train_ds = IAMData('train',transform = transforms,target_transform= target_transforms)\n","\n","\n","\n"],"metadata":{"id":"ruAcm-qU1xUs"},"execution_count":null,"outputs":[]}]}